{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xaig6vNzjN5A"
   },
   "outputs": [],
   "source": [
    "import os, matplotlib, pickle, six\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import pandas as pd\n",
    "from imgaug import augmenters as iaa\n",
    "import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Activation, Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D, SeparableConv2D\n",
    "from keras.layers.merge import add\n",
    "from keras import regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from imageio import imread\n",
    "from keras.callbacks import *\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "batch_size = 256\n",
    "nb_class = 200\n",
    "nb_epoch = 300\n",
    "img_per_class = 500\n",
    "\n",
    "# input image dimensions\n",
    "img_height, img_width = 64, 64\n",
    "channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "phm86ZULdPa1"
   },
   "outputs": [],
   "source": [
    "!wget 'http://cs231n.stanford.edu/tiny-imagenet-200.zip'\n",
    "!unzip -qq '/tiny-imagenet-200.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wRYLyZtwKKDp"
   },
   "outputs": [],
   "source": [
    "# Image Augmentation\n",
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "seq = iaa.Sequential([\n",
    "                      iaa.GaussianBlur(sigma=(0, 1.0)),\n",
    "                      iaa.Fliplr(0.5),\n",
    "                      iaa.Crop(percent=(0, 0.2)),\n",
    "                      iaa.CoarseDropout(p=0.1, size_percent=(0.15, 0.3)),\n",
    "                              sometimes(iaa.Affine(                                                                                                       \n",
    "                                                    scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "                                                    translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "                                                    rotate=(-30, 30),\n",
    "                                                    shear=(-16, 16)))\n",
    "      ])\n",
    "\n",
    "# Use Train Generator and Validation Genetator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale= 1./255,\n",
    "    preprocessing_function=seq.augment_image\n",
    "    )\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "igYtU_VSKXto",
    "outputId": "f90459cf-0df1-4519-a533-ca189147e409"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 images belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory( r'./tiny-imagenet-200/train/', \n",
    "                                                    target_size=(img_height, img_width),\n",
    "                                                    color_mode='rgb',\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=True, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "UBRfC4SdKbca",
    "outputId": "60aeb84e-5ba2-4ba3-8ec4-c9b5f42ddaf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 200 classes.\n",
      "Training/Validation Data is created\n"
     ]
    }
   ],
   "source": [
    "val_data = pd.read_csv('./tiny-imagenet-200/val/val_annotations.txt', sep='\\t', header=None, names=['File', 'Class', 'X', 'Y', 'H', 'W'])\n",
    "val_data.drop(['X', 'Y', 'H', 'W'], axis=1, inplace=True)\n",
    "\n",
    "validation_generator = valid_datagen.flow_from_dataframe(val_data, directory='./tiny-imagenet-200/val/images/', \n",
    "                                                         x_col='File', y_col='Class',\n",
    "                                                         target_size=(img_height, img_width),\n",
    "                                                         color_mode='rgb', class_mode='categorical', \n",
    "                                                         batch_size=batch_size, shuffle=False, seed=None)    # Make shuffle false\n",
    "\n",
    "def load_tiny_imagenet(path):\n",
    "  \n",
    "  # First load wnids\n",
    "  wnids_file = os.path.join(path, 'wnids.txt')\n",
    "  with open(os.path.join(wnids_file), 'r') as f:\n",
    "    wnids = [x.strip() for x in f]\n",
    "  wnids.sort()\n",
    "  \n",
    "  # Map wnids to integer labels\n",
    "  wnid_to_label = {wnid: i for i, wnid in enumerate(wnids)}\n",
    "\n",
    "  # Use words.txt to get names for each class\n",
    "  words_file = os.path.join(path, 'words.txt')\n",
    "  with open(os.path.join(words_file), 'r') as f:\n",
    "    wnid_to_words = dict(line.split('\\t') for line in f)\n",
    "      \n",
    "  return wnids, wnid_to_label, wnid_to_words\n",
    "\n",
    "wnids, wnid_to_label, wnid_to_words = load_tiny_imagenet(path='tiny-imagenet-200/')\n",
    "print('Training/Validation Data is created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "axstpji8sNof",
    "outputId": "abd1dc63-e824-4141-bfb7-3799b47db0c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Shape:  (128, 64, 64, 3)\n",
      "y_train Shape:  (128, 200)\n",
      "X_test Shape:  (128, 64, 64, 3)\n",
      "y_test Shape:  (128, 200)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = next(train_generator)\n",
    "X_test, y_test = next(validation_generator)\n",
    "\n",
    "print(\"X_train Shape: \",X_train.shape)\n",
    "print(\"y_train Shape: \",y_train.shape)\n",
    "print(\"X_test Shape: \",X_test.shape)\n",
    "print(\"y_test Shape: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_yFX4PDqf5X5"
   },
   "outputs": [],
   "source": [
    "def bn_relu(input):\n",
    "    norm = BatchNormalization()(input)\n",
    "    return Activation(\"relu\")(norm)\n",
    "  \n",
    "def conv_block(input_shape, filters, bottleneck, repetition=2, padding='same', non_residual=False):\n",
    "  for i in range(repetition):\n",
    "    if i == 0:\n",
    "      activation = bn_relu(input_shape)\n",
    "    else:\n",
    "      activation = bn_relu(convolution)\n",
    "    convolution = SeparableConv2D(filters=filters, kernel_size=(3,3), padding=padding, kernel_regularizer=regularizers.l2(0.01))(activation)      #kernel_regularizer=regularizers.l1(0.01)\n",
    "    filters *= 2\n",
    "  \n",
    "  if non_residual==True:\n",
    "    return Conv2D(filters=bottleneck, kernel_size=(1,1), activation='relu')(convolution)\n",
    "  if input_shape.shape[3] != convolution.shape[3]:\n",
    "    input_shape = Conv2D(filters=int(convolution.shape[3]), kernel_size=(1,1))(input_shape)\n",
    "  \n",
    "  concatenate = add([input_shape, convolution])\n",
    "  \n",
    "  if bottleneck==0:\n",
    "    return concatenate\n",
    "  else:\n",
    "    output_shape = Conv2D(filters=bottleneck, kernel_size=(1,1), activation='relu')(concatenate)\n",
    "    return output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2428
    },
    "colab_type": "code",
    "id": "-vzaQ3ST7ZS5",
    "outputId": "b00c3dc3-21e2-4108-db2e-e375b60b566c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, None, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, None, 3 12          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, None, 3 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, None, None, 3 155         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, None, 3 128         separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, None, 3 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, None, None, 6 256         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, None, None, 6 2400        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, None, None, 6 0           conv2d_1[0][0]                   \n",
      "                                                                 separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, None, 6 256         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, None, None, 6 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, None, None, 6 4736        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, None, None, 6 256         separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, None, None, 6 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, None, None, 1 8320        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_4 (SeparableCo (None, None, None, 1 8896        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, None, None, 1 0           conv2d_2[0][0]                   \n",
      "                                                                 separable_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, None, None, 1 512         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, None, None, 1 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_5 (SeparableCo (None, None, None, 1 17664       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, None, None, 1 512         separable_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, None, None, 1 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, None, None, 2 33024       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_6 (SeparableCo (None, None, None, 2 34176       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, None, None, 2 0           conv2d_3[0][0]                   \n",
      "                                                                 separable_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, None, None, 1 32896       add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, None, None, 1 0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, None, 1 512         max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, None, None, 1 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_7 (SeparableCo (None, None, None, 1 17664       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, None, None, 1 512         separable_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, None, None, 1 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, None, None, 2 33024       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_8 (SeparableCo (None, None, None, 2 34176       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, None, None, 2 0           conv2d_5[0][0]                   \n",
      "                                                                 separable_conv2d_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, None, None, 2 1024        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, None, None, 2 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_9 (SeparableCo (None, None, None, 2 68096       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, None, None, 2 1024        separable_conv2d_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, None, None, 2 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, None, None, 5 131584      add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_10 (SeparableC (None, None, None, 5 133888      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, None, None, 5 0           conv2d_6[0][0]                   \n",
      "                                                                 separable_conv2d_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, None, None, 5 0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, None, None, 5 2048        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, None, None, 5 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_11 (SeparableC (None, None, None, 5 267264      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, None, None, 5 2048        separable_conv2d_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, None, None, 5 0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, None, None, 1 525312      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_12 (SeparableC (None, None, None, 1 529920      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, None, None, 1 0           conv2d_7[0][0]                   \n",
      "                                                                 separable_conv2d_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, None, None, 5 524800      add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, None, None, 5 2048        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, None, None, 5 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_13 (SeparableC (None, None, None, 1 529920      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, None, None, 1 4096        separable_conv2d_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, None, None, 1 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_14 (SeparableC (None, None, None, 2 2108416     activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, None, None, 2 409800      separable_conv2d_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 200)          0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 200)          0           global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 5,471,375\n",
      "Trainable params: 5,463,881\n",
      "Non-trainable params: 7,494\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(None, None, channels))               # img_height, img_width\n",
    "\n",
    "block1 = conv_block(input_shape=input, filters=32, bottleneck=0)\n",
    "block2 = conv_block(input_shape=block1, filters=64, bottleneck=0)\n",
    "block3 = conv_block(input_shape=block2, filters=128, bottleneck=128)\n",
    "\n",
    "block4 = MaxPooling2D(pool_size=(2,2))(block3)\n",
    "\n",
    "block5 = conv_block(input_shape=block4, filters=128, bottleneck=0)\n",
    "block6 = conv_block(input_shape=block5, filters=256, bottleneck=0)\n",
    "\n",
    "block7 = MaxPooling2D(pool_size=(2,2))(block6)\n",
    "\n",
    "block8 = conv_block(input_shape=block7, filters=512, bottleneck=512)\n",
    "block9 = conv_block(input_shape=block8, filters=1024, bottleneck=200, repetition=2, non_residual=True)\n",
    "\n",
    "output = GlobalAveragePooling2D()(block9)\n",
    "output = Activation('softmax')(output)\n",
    "\n",
    "model = Model(inputs=input, outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HK-ckec4erpH",
    "outputId": "f719f573-d4a0-454e-d0bb-d1bdd4646959"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model/Weights from previous session loaded\n"
     ]
    }
   ],
   "source": [
    "model= load_model('gdrive/My Drive/EIP/take3_final.h5')           #  = load_model    .load_weights\n",
    "print('Model/Weights from previous session loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oe-N_wr7jjph"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('gdrive/My Drive/EIP/take3_final.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "lr_reducer = ReduceLROnPlateau(monitor='acc', mode='auto', factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
    "csv_logger = CSVLogger('gdrive/My Drive/EIP/resnet_tiny2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mwkLJF3gNraN"
   },
   "source": [
    "## Class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bsFGTIOxegei"
   },
   "outputs": [],
   "source": [
    "# Method 2:\n",
    "\n",
    "y_pred = model.predict_generator(validation_generator, steps=79) # Steps 20 only if batch_size = 512\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_true = validation_generator.classes\n",
    "\n",
    "actual_words = []\n",
    "for i in val_data.Class:\n",
    "  actual_words.append(wnid_to_words[i])\n",
    "  \n",
    "df = pd.DataFrame({'File Name': val_data.File, 'actual label': val_data.Class, 'actual words': actual_words, 'actual pred': y_true, 'predictions': y_pred}, \n",
    "                  columns=['File Name', 'actual label', 'actual words', 'actual pred', 'predictions'])\n",
    "df.to_csv('gdrive/My Drive/EIP/prediction_classwise.csv')\n",
    "\n",
    "df['correct'] = np.where(df['actual pred'] == df['predictions'], df['actual label'], 'Nan')\n",
    "hard_df = df.loc[df['correct'] != 'Nan']\n",
    "\n",
    "hard_df_count = hard_df.groupby(['actual pred']).size().reset_index(name='count')\n",
    "dict_labels_vs_samples = hard_df_count.set_index('actual pred').T.to_dict('index').pop('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------NOT REQUIRED------------------------------------\n",
    "# Move Images if required to check the model is performing badly on a few datasets\n",
    "import shutil\n",
    "hard_images = hard_df['File Name'].tolist()\n",
    "for f in hard_images:\n",
    "  shutil.move('./tiny-imagenet-200/val/images/{}'.format(f), 'gdrive/My Drive/EIP/hard_classwise/{}'.format(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "iEwsok3JMaQF",
    "outputId": "ffdb02b4-00da-4ac1-b752-dab8cf0238e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_samples:  5152\n",
      "num_classes:  200\n",
      "max_of_all_classes:  44\n",
      "multiplying_factor:  1.7080745341614907\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------ONLY FOR CLASS WEIGHTS--------------------------\n",
    "def get_class_weights(dict_labels_vs_samples, balanced=True):\n",
    "  \n",
    "  if not len(dict_labels_vs_samples) == 200:\n",
    "    print('WARNING: dict passed is NOT of length 200 - meaning not all 200 classes are included in the dict')\n",
    "    \n",
    "  keys = dict_labels_vs_samples.keys()\n",
    "  values = list(dict_labels_vs_samples.values())\n",
    "  total_samples = sum(values)\n",
    "  num_classes = len(values)\n",
    "  max_of_all_classes = max(values)\n",
    "  average_of_all_classes = total_samples / num_classes\n",
    "  multiplying_factor = 1\n",
    "  \n",
    "  if balanced:\n",
    "    multiplying_factor = max_of_all_classes / average_of_all_classes\n",
    "  \n",
    "  print('total_samples: ', total_samples)\n",
    "  print('num_classes: ', num_classes)\n",
    "  print('max_of_all_classes: ', max_of_all_classes)\n",
    "  print('multiplying_factor: ', multiplying_factor)\n",
    "  \n",
    "  class_weight = dict()\n",
    "\n",
    "  for key in keys:\n",
    "        num_correct_pred = int(dict_labels_vs_samples.get(key))\n",
    "        if num_correct_pred == 0:\n",
    "          num_correct_pred = 1 # this is to avoid divide by zero error, if a class has no correct predictions\n",
    "        score_for_class = (total_samples / (num_classes * num_correct_pred)) * multiplying_factor\n",
    "        class_weight[key] = score_for_class\n",
    "        \n",
    "  return class_weight\n",
    "\n",
    "class_weights = get_class_weights(dict_labels_vs_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "ylU5EJ-n_NYG",
    "outputId": "fb662101-194d-43d3-acf0-331c817ba8c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 1s 11ms/step\n",
      "Test Accuracy:  0.578125\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test Accuracy: ', score[1])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Ah1V9Rs8q41M"
   ],
   "name": "A4_Resiudal.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
